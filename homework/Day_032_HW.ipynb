{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 練習時間\n",
    "相信大家目前都能夠初步了解機器學習專案的流程及步驟，今天的作業希望大家能夠看看全球機器學習巨頭們在做的機器學習專案。以 google 為例，下圖是 Google 內部專案使用機器學習的數量，隨著時間進展，現在早已超過 2000 個專案在使用 ML。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](https://cdn-images-1.medium.com/max/800/1*U_L8qI8RmYS-MOBrYvXhSA.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "底下幫同學整理幾間知名企業的 blog 或機器學習網站 (自行搜尋也可)，請挑選一篇文章閱讀並試著回答\n",
    "1. 專案的目標？ (要解決什麼問題）\n",
    "2. 使用的技術是？ (只需知道名稱即可，例如：使用 CNN 卷積神經網路做影像分類)\n",
    "3. 資料來源？ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Google AI blog](https://ai.googleblog.com/)\n",
    "- [Facebook Research blog](https://research.fb.com/blog/)\n",
    "- [Apple machine learning journal](https://machinelearning.apple.com/)\n",
    "- [機器之心](https://www.jiqizhixin.com/)\n",
    "- [雷鋒網](http://www.leiphone.com/category/ai)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "google ai blog Moving Camera, Moving People: A Deep Learning Approach to Depth Prediction\n",
    "1.Our predicted depth maps can be used to produce a range of 3D-aware video effects. \n",
    "Other possible applications for our depth maps include generating a stereo video from a monocular one, and inserting synthetic CG objects into the scene.\n",
    "\n",
    "2.The Mannequin Challenge videos provide depth supervision for moving camera and “frozen” people, \n",
    "but our goal is to handle videos with a moving camera and moving people. \n",
    "We need to structure the input to the network in order to bridge that gap. \n",
    "A possible approach is to infer depth separately for each frame of the video (i.e., the input to the model is just a single frame). \n",
    "While such a model already improves over state-of-the-art single image methods for depth prediction, \n",
    "we can improve the results further by considering information from multiple frames.\n",
    "For example, motion parallax, i.e., the relative apparent motion of static objects between two different viewpoints, \n",
    "provides strong depth cues. To benefit from such information, \n",
    "we compute the 2D optical flow between each input frame and another frame in the video, \n",
    "which represents the pixel displacement between the two frames. \n",
    "This flow field depends on both the scene’s depth and the relative position of the camera. \n",
    "However, because the camera positions are known, we can remove their dependency from the flow field, \n",
    "which results in an initial depth map. This initial depth is valid only for static scene regions. \n",
    "To handle moving people at test time, we apply a human-segmentation network to mask out human regions in the initial depth map. \n",
    "The full input to our network then includes: the RGB image, the human mask, and the masked depth map from parallax.\n",
    "\n",
    "3.we make use of an existing source of data for supervision: \n",
    "YouTube videos in which people imitate mannequins by freezing in a wide variety of natural poses,\n",
    "while a hand-held camera tours the scene. Because the entire scene is stationary (only the camera is moving),\n",
    "triangulation-based methods--like multi-view-stereo (MVS)--work, \n",
    "and we can get accurate depth maps for the entire scene including the people in it.\n",
    "We gathered approximately 2000 such videos, spanning a wide range of realistic scenes with people naturally posing in different group configurations. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
